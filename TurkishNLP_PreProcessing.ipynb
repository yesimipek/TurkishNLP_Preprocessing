{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TurkishNLP_PreProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLbJ6eKjSrFA"
      },
      "source": [
        "References: \n",
        "* Clean Messages Title and Replace Emoticon title are editted from following code : https://www.kaggle.com/egebasturk1/yemeksepeti-sentiment-analysis/comments#The-Lemmatizer \n",
        "* Merge CSV Title is editted from following page:\n",
        "https://www.freecodecamp.org/news/how-to-combine-multiple-csv-files-with-8-lines-of-code-265183e0854/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8-Nv0t4ToD5",
        "outputId": "c9f224bf-35c9-4e87-b8fa-ceb7563c007f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #connect your drive to notebook"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x2PRHIvTxLu"
      },
      "source": [
        "Frameworks and packages that are used in this Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj2x-5MVV3Hp",
        "outputId": "c18fbcc7-c84a-41a6-d6ac-e94f58deacee"
      },
      "source": [
        "pip install jpype1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jpype1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/af/93f92b38ec1ff3091cd38982ed19cea2800fefb609b5801c41fc43c0781e/JPype1-1.2.1-cp36-cp36m-manylinux2010_x86_64.whl (457kB)\n",
            "\r\u001b[K     |▊                               | 10kB 24.7MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 15.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 9.5MB/s eta 0:00:01\r\u001b[K     |██▉                             | 40kB 8.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 61kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 71kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 81kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 92kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 102kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 112kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 122kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 133kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 143kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 153kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 163kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 174kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 184kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 194kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 204kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 215kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 225kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 235kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 245kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 256kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 266kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 276kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 286kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 296kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 307kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 317kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 327kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 337kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 348kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 358kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 368kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 378kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 389kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 399kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 409kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 419kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 430kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 440kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 450kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 460kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jpype1) (3.7.4.3)\n",
            "Installing collected packages: jpype1\n",
            "Successfully installed jpype1-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9zXnLdpwuBJ"
      },
      "source": [
        "import jpype\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation, digits\n",
        "import itertools\n",
        "from pathlib import Path\n",
        "import re\n",
        "import os\n",
        "from typing import List\n",
        "from jpype import JClass, JString, getDefaultJVMPath, shutdownJVM, startJVM, java\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import nltk\n",
        "import re\n",
        "from string import punctuation, digits\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "nltk.download(\"all\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "788L1e5jw1iM"
      },
      "source": [
        "# Reading CSV - DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VvIl1wBxAPl"
      },
      "source": [
        "#While reading the csv file into df you may need to determine delimeter different than comma\n",
        "df = pd.read_csv(\"path_of_csv_file.csv\", delimiter = \"~\", encoding='latin-1') \n",
        "## also some turkish letters in some datasets are changed into other letters\n",
        "replaceDict={'Ý':'İ','Ð':'Ğ','Þ':'Ş','þ':'ş','ý':'ı','ð':'ğ'} \n",
        "\n",
        "for x in replaceDict:\n",
        "    df=df.replace(x,replaceDict[x],regex=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM4Tcgmc2baP"
      },
      "source": [
        "df[\"raw_text_column\"]=df[\"raw_text_column\"].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkLA6ASpxAc8"
      },
      "source": [
        "# Replace Emoticon - Remove Links&Digits&Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdpJPsEzABU"
      },
      "source": [
        "# Especially the social media texts have several emoji, we should validate those emoji's meaning as text.\n",
        "def replaceemoticon(word):\n",
        "    \n",
        "    pos = re.findall(r\"(?::\\) | :-\\) | =\\) | :D | :d | <3 | \\(: | :\\'\\) | \\^\\^|;\\) | \\(-:)\", word)\n",
        "    neg = re.findall(r\"(:-\\( | :\\( | ;\\( | ;-\\( | =\\( | :/ | :\\\\ | -_- | \\): | \\)-:)\", word)\n",
        "  \n",
        "    if pos:\n",
        "        #word = \":)\"\n",
        "        word = word + \" pozitif\"\n",
        "\n",
        "    elif neg:\n",
        "        #word = \":(\"\n",
        "        word = word + \" negatif\"\n",
        "\n",
        "    return word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZvJ5xxEzFAz"
      },
      "source": [
        "df[\"preProcessing\"] = df[\"raw_text_column\"].map(replaceemoticon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4-FWN3dxEbs"
      },
      "source": [
        "def preProcessing(text):\n",
        "\n",
        "  return resubsatır(df_remove_punc(df_remove_digits(unlem(df_remove_email_adressed(df_remove_com_tr(df_remove_link(text)))))))\n",
        "\n",
        "def df_remove_link(text):\n",
        "  return re.sub(r\"((http\\S+)|(www\\S+))\", \" \", text)\n",
        "\n",
        "def df_remove_com_tr(text):\n",
        "    text = re.sub(r'\\S*\\.(com|tr)',\" \",text)\n",
        "    return text\n",
        "\n",
        "def df_remove_email_adressed(text):\n",
        "  return re.sub(r'([.\\w]{3,}@[.\\w]{5,})', ' ', text)\n",
        "\n",
        "def df_remove_digits(text): #remove 0123456789\n",
        "    text=text.strip()\n",
        "    remove_digits = str.maketrans(' ', ' ', digits)\n",
        "    return text.translate(remove_digits)\n",
        "\n",
        "def unlem(text): # '!' punctuation may have some sentiment meaning, that's why we turn it as text.\n",
        "    if \"!\" in text:\n",
        "      return text+\" ünlem\"\n",
        "    else:\n",
        "      return text\n",
        "\n",
        "def resubsatır(text): #remove \\n \n",
        "    texts= re.sub(\"\\n\",\" \",text)\n",
        "    return texts\n",
        "\n",
        "def df_remove_punc(text): #remove punctuation\n",
        "    regex = re.compile('[%s]' % re.escape(punctuation))\n",
        "    return regex.sub(' ', text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMm8HSN1xE11"
      },
      "source": [
        "%%time\n",
        "## Remove http, com, link, digits:\n",
        "df[\"preProcessing\"] = df[\"preProcessing\"].map(preProcessing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTD2AfOh0Xj7"
      },
      "source": [
        "def replace_a(text): #replace â to a \n",
        "    if \"â\" in text:\n",
        "      return text+\"a\"\n",
        "    else:\n",
        "      return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6XE9Bny0ZAu"
      },
      "source": [
        "df[\"preProcessing\"] = df[\"preProcessing\"].map(replace_a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZzV1FPjxHR8"
      },
      "source": [
        "# Editting the dataset content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKqFpMaNxJ-2"
      },
      "source": [
        "data=df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHc4khcTxKBg"
      },
      "source": [
        "pd.set_option(\"display.max_rows\", data.shape[0])\n",
        "pd.set_option('display.max_colwidth', data.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6DJTke-aNHU"
      },
      "source": [
        "Lower the text letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAyxcmn_rnnU"
      },
      "source": [
        "def lowers(text):\n",
        "    text=re.sub(\"I\",\"ı\",text)\n",
        "    text=re.sub(\"İ\",\"i\",text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "data[\"preProcessing\"] = data[\"preProcessing\"].map(lowers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqWdHCizaxqG"
      },
      "source": [
        "Deleting duplicates and NA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9qlhzBOrnnV"
      },
      "source": [
        "#Drop all dupplicates in the TICKET_DESC column\n",
        "\n",
        "data.drop_duplicates(subset=['preProcessing'], inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrmMWo5hVTeT"
      },
      "source": [
        "# Drop the NA data\n",
        "\n",
        "data.dropna(subset = [\"preProcessing\"], inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13vHDxMLaThq"
      },
      "source": [
        "(if needed) Deleting some terms that you dont want to in your dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lYOr1r-ABOy"
      },
      "source": [
        "# bilgi verildi geçen verileri çıkartalım \n",
        "data = data[~data.preProcessing.str.contains(\" bilgi verildi\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igf2TeYLaWus"
      },
      "source": [
        "Elimating too short and too long text length "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw0GtJEVVlPT"
      },
      "source": [
        "# her satırdaki mesajın kelime sayısını 'totalwords' sütununa yaz\n",
        "data['totalwords'] = data['preProcessing'].str.split().str.len()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el3LJJjFo400"
      },
      "source": [
        "# You can determine the word length according to mean and standard deviation of word length in dataset\n",
        "data = data[data.totalwords > 6]\n",
        "data = data[data.totalwords < 61]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRYgDmWixMwW"
      },
      "source": [
        "# Word Tokenize "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhhMz7lc0OTN",
        "outputId": "b5ea12d9-4407-4b94-c070-b5a6ff49e703"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h20oLX250OTN"
      },
      "source": [
        "data['preProcessing'] = data.apply(lambda row: word_tokenize(row['preProcessing']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu-NC-4MxOxO"
      },
      "source": [
        "data.iloc[100:110]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfPrpmvywpVu"
      },
      "source": [
        "# Tokenize to String"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSts7Medwg1n"
      },
      "source": [
        "def tokenize_to_str(text):\n",
        "  text= re.sub(r\"'\",\"\", text)\n",
        "  text= re.sub(r\",\",\"\", text)\n",
        "  text= re.sub(r\"\\[\",\"\", text)\n",
        "  text= re.sub(r\"\\]\",\"\", text)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAEbxv4F4rnm"
      },
      "source": [
        "def tokenizeList_to_str(text):\n",
        "  return \" \".join(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP-ZKfAH0LcO"
      },
      "source": [
        "data['preProcessing'] = data['preProcessing'].map(tokenizeList_to_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhsAoE7IxRll"
      },
      "source": [
        "# Deleting speacial characters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPpKIG2Tbkdu"
      },
      "source": [
        "Deleting speacial characters that different from alphabet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVi5Zayk0aO0"
      },
      "source": [
        "karakterler=[]\n",
        "def find(text):\n",
        "  for i in text:\n",
        "    if(i not in [\"q\",\"x\",\"w\",\"a\",\"b\",\"c\",\"ç\",\"d\",\"e\",\"f\",\"g\",\"ğ\",\"h\",\"ı\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"ö\",\"p\",\"r\",\"s\",\"ş\",\"t\",\"u\",\"ü\",\"v\",\"y\",\"z\"]):\n",
        "      karakterler.append(i)\n",
        "    else:\n",
        "      continue\n",
        "data[\"preProcessing\"].map(find)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh_zW9bS4_NP",
        "outputId": "37cc900a-4bd9-4410-8b95-d1a910a361e5"
      },
      "source": [
        "silinecek_unique = set(karakterler)\n",
        "print(silinecek_unique)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ß', 'ô', '¶', '\\x91', '¿', 'ì', 'ø', 'ï', '¾', '¸', 'ä', '«', 'ê', 'ú', '£', 'á', 'û', '°', '×', 'à', '±', 'ó', 'é', 'å', '·', '÷', '´', 'í', '\\x92', '¨', 'ë', 'â', '²', '\\x7f', 'æ', 'ã', 'î', '½', 'ñ', '¼', '§', 'õ', 'ù', 'è', 'ò', ' ', '³', '»', '¹', '©'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X4JX9Qh0aEx"
      },
      "source": [
        "silinecek_unique = set(karakterler)\n",
        "for i in silinecek_unique:\n",
        "  data[\"preProcessing\"]=data[\"preProcessing\"].str.replace(i,\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym3XnDxnwhXM"
      },
      "source": [
        "# Deleting one character in text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ4W4bm7wY1g"
      },
      "source": [
        "#### We are deleting one character remained in the text not two character because in Turkish you have word examples like \"et,su,ev\"\n",
        "def sil_1(text):\n",
        "    text_list=text.split()\n",
        "    for i in text_list:\n",
        "        if len(i)==1:\n",
        "            text_list.remove(i)   \n",
        "    return \" \".join(text_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFsV6Zdj5cGh"
      },
      "source": [
        "data[\"preProcessing\"]=data[\"preProcessing\"].map(sil_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-x6av6owZII"
      },
      "source": [
        "# Deleting Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFtu9UxWOXfk"
      },
      "source": [
        "WPT = nltk.WordPunctTokenizer()\n",
        "stop_word_list = nltk.corpus.stopwords.words('turkish')\n",
        "stop_word_list.remove('ama') ##you can edit stopwords list- we don't want to delete \"ama\" because it has meaningful negativity in Turkish."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyyWi2jMwL_t"
      },
      "source": [
        "def stopwords_sil(text):\n",
        "    text_list=text.split()\n",
        "    silinmis_list = [text for text in text_list if text not in stop_word_list]\n",
        "    silinmis_list=\" \".join(silinmis_list)\n",
        "    return silinmis_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3EdSgDuxggv"
      },
      "source": [
        "data[\"preProcessing\"]=data[\"preProcessing\"].map(stopwords_sil)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO1NYk3AxhRl"
      },
      "source": [
        "# Deleting English words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoiWCEmFwgi_"
      },
      "source": [
        "pip install fasttext #for detecting language "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZHNET6bCqct"
      },
      "source": [
        "import fasttext\n",
        "PRETRAINED_MODEL_PATH = 'drive/MyDrive/..path../lid.176.bin'\n",
        "model = fasttext.load_model(PRETRAINED_MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0KkQOnf8_jz"
      },
      "source": [
        "def ing(text):\n",
        "    predictions = model.predict([text], k=2) #it will give you the most possible two languge label\n",
        "    if predictions[0][0][0] ==\"__label__tr\":\n",
        "        y=predictions[1][0][0]\n",
        "        if y > 0.92: #you can determine the turkish language score according to your dataset\n",
        "            return \"tr\" #this will labelled the rows as \"tr\" or \"NaN\" according to their lang score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY6Ov2C9GohX"
      },
      "source": [
        "data[\"lang\"]=data[\"preProcessing\"].map(ing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSkwWLzxG4a6",
        "outputId": "6f7c3d9a-df2b-48e7-e099-79315b069056"
      },
      "source": [
        "tr_olmayan_df=data[data[\"lang\"]!=\"tr\"] ## how many rows are not Turkish?\n",
        "tr_olmayan_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1484, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt8AoSM3EY4B"
      },
      "source": [
        "data_ingsiz=data.dropna(subset=['lang']) #deleting NaN valued,which in Lang column, rows  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sWtlswjPsLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1eea134-08bb-41b2-ebe6-a9b362aebea6"
      },
      "source": [
        "data_ingsiz.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(830240, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQNSzX6kPsQC"
      },
      "source": [
        "data_ingsiz.to_csv(r'drive/My Drive/..yourpath../PreProcessing_pt1.csv', index=False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhO-ir48t3xq"
      },
      "source": [
        "# Sampling from big df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4EuJ_MluAvA"
      },
      "source": [
        "dataP = pd.read_csv(r'drive/My Drive/..yourpath../PreProcessing_pt1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcaEMhH7uDWD"
      },
      "source": [
        "dataP['COUNTY_column'].value_counts().to_frame() ##for example : let's do sampling on preprocessed text according to each counties in İstanbul "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWTI90AzwKjH"
      },
      "source": [
        "dataP['COUNTY_column'].value_counts().index.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r__inBNPwMqS"
      },
      "source": [
        "##İstanbul  counties' list\n",
        "ist_ilce = ['ADALAR', 'ARNAVUTKÖY', 'ATAŞEHİR', 'AVCILAR', 'BAĞCILAR', 'BAHÇELİEVLER', 'BAKIRKÖY', 'BAŞAKŞEHİR', 'BAYRAMPAŞA', 'BEŞİKTAŞ', 'BEYKOZ', 'BEYLİKDÜZÜ', 'BEYOĞLU', 'BÜYÜKÇEKMECE', 'ÇATALCA', 'ÇEKMEKÖY', 'ESENLER', 'ESENYURT', 'EYÜPSULTAN', 'FATİH', 'GAZİOSMANPAŞA', 'GÜNGÖREN', 'KADIKÖY', 'KAĞITHANE', 'KARTAL', 'KÜÇÜKÇEKMECE', 'MALTEPE', 'PENDİK', 'SANCAKTEPE', 'SARIYER', 'ŞİŞLİ', 'ŞİLE', 'SİLİVRİ', 'SULTANBEYLİ', 'SULTANGAZİ', 'TUZLA', 'ÜMRANİYE', 'ÜSKÜDAR', 'ZEYTİNBURNU']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVaSKyYvwPhv",
        "outputId": "7761b208-f1b3-4fd8-b634-8b2fba3b2461"
      },
      "source": [
        "len(ist_ilce)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-pxeqJfwZXW"
      },
      "source": [
        "df2 = dataP[dataP['COUNTY_column'].isin(ist_ilce)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDRg2fbuwoDK",
        "outputId": "3ba942a1-741b-4c93-ebe5-ef859680adf5"
      },
      "source": [
        "df2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(827985, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGA2EgKHw1on",
        "outputId": "921befaf-268b-4024-b485-aaf9c45aa538"
      },
      "source": [
        "df2['COUNTY_column'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      ZEYTİNBURNU\n",
              "1          AVCILAR\n",
              "2    GAZİOSMANPAŞA\n",
              "3       ARNAVUTKÖY\n",
              "4       ARNAVUTKÖY\n",
              "Name: X_IBB_COUNTY, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0W5K8CyxYjL"
      },
      "source": [
        "A=df2['COUNTY_column'].value_counts() #check-> how many county is valid in your dataset\n",
        "## edit ist_ilce list according to this"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPSbyvEa3gKK"
      },
      "source": [
        "def ayrıdf(i):\n",
        "  filter= df2['COUNTY_column'].str.contains(i, na=False )\n",
        "  df3=df2.loc[filter]\n",
        "  aa=pd.DataFrame(df3)\n",
        "  b= df2.shape[0]/ {determined combined samples row shape}\n",
        "  count=int(int(A.loc[i])/b)\n",
        "  sampled=aa[\"Text_ID\"].sample(n= count) #unique ID \n",
        "  df_pre= df_i.loc[sampled]\n",
        "  df_pre.reset_index(inplace=True)\n",
        "  return  df_pre.to_csv(r'drive/My Drive/..path../{0}.csv'.format(i), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymgjv6Nr3n5d"
      },
      "source": [
        "for i in ist_ilce:\n",
        "  ayrıdf(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s2jqh-n44DA"
      },
      "source": [
        "# Merge the little sample df's into one df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZCcFK9s4-Dz"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "os.chdir(\"drive/My Drive/..path../{foldername}\") #give the folder path of the sample dataframes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_m0CPhf_EM6"
      },
      "source": [
        "extension = 'csv'\n",
        "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLfRcN2B_IaV"
      },
      "source": [
        "#combine all files in the list\n",
        "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
        "#export to csv\n",
        "combined_csv.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8-sig')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeTpyCM6_K-1"
      },
      "source": [
        "import pandas as pd\n",
        "comb=pd.read_csv(\"drive/My Drive/ilce/combined_csv.csv\") #write the path to save combined_csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "somvCTGL_Lua"
      },
      "source": [
        "comb.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUmXut0__N5G"
      },
      "source": [
        "comb['COUNTY_column'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2itVzXiwMwZ"
      },
      "source": [
        "# Clean Messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R89F9MQrgWCA"
      },
      "source": [
        "We are using Zemberek packages for lemmatization, Turkish morphology , normalizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMGTHgnSruEV"
      },
      "source": [
        "import jpype #you may need to download java program if you are working on your local\n",
        "from jpype import getDefaultJVMPath,JClass\n",
        "jar = r\"drive/MyDrive/..path../zemberek-full.jar\"\n",
        "from jpype import getDefaultJVMPath,JClass\n",
        "if not jpype.isJVMStarted():\n",
        "    jpype.startJVM(getDefaultJVMPath(), classpath=[jar])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTDcFTeBdlG9",
        "outputId": "11fa2135-aeaa-4746-ad6b-bd0bd2c5501f"
      },
      "source": [
        "ZEMBEREK_PATH = r'drive/MyDrive/..path../zemberek-full.jar'\n",
        "TurkishMorphology = JClass('zemberek.morphology.TurkishMorphology')\n",
        "TurkishSpellChecker = JClass('zemberek.normalization.TurkishSpellChecker')\n",
        "TurkishSentenceNormalizer = JClass('zemberek.normalization.TurkishSentenceNormalizer')\n",
        "Paths = JClass('java.nio.file.Paths')\n",
        "lookupRoot = Paths.get(\"drive/MyDrive/NLP/Kutuphaneler, Kaynaklar/normalization\")\n",
        "lmPath = Paths.get(\"drive/MyDrive/..path../data/lm/lm.2gram.slm\")\n",
        "morphology = TurkishMorphology.createWithDefaults()\n",
        "morph = TurkishMorphology.createWithDefaults()\n",
        "spell = TurkishSpellChecker(morph)\n",
        "#you need to download lookuproot, lmp.2gram, zemberek_full.jar files in your drive "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7.52 s, sys: 355 ms, total: 7.87 s\n",
            "Wall time: 4.12 s\n",
            "Parser   : 912 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veMABjrAovXk"
      },
      "source": [
        "WPT = nltk.WordPunctTokenizer()\n",
        "stop_word_list = nltk.corpus.stopwords.words('turkish')\n",
        "stop_word_list.remove('ama')\n",
        "WORDS = dict()\n",
        "spell_folder = Path(r\"{write_path}\") \n",
        "#download big2.txt folder from https://www.kaggle.com/egebasturk1/yemeksepeti-sentiment-analysis/comments?select=big2.txt\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "with open(os.path.expanduser(Path(spell_folder/ \"big2.txt\")), \"r\", encoding = 'utf-8') as f:\n",
        "    for line in f:\n",
        "        splitted = line.split()\n",
        "        WORDS[splitted[0]] = int(splitted[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9fJMMVPovac"
      },
      "source": [
        "def reverse(s): \n",
        "    if len(s) == 0: \n",
        "        return s \n",
        "    else: \n",
        "        return reverse(s[1:]) + s[0] \n",
        "\n",
        "def checkOpennes(word):\n",
        "    vowels=['a','e','i','ı','o','ö','u','ü']\n",
        "    open_vowels=['e','i','ü','ö']\n",
        "    close_vowels=['a','ı','o','u']\n",
        "    for i in range(len(word)):\n",
        "        if reverse(word)[i] in vowels:\n",
        "            if reverse(word)[i] in open_vowels:\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "        else:\n",
        "            continue\n",
        "def PresentCheck(word):\n",
        "        ei=['e','i']\n",
        "        aı=['a','ı']\n",
        "        üö=['ü','ö']\n",
        "        uo=['u','o']\n",
        "        for i in range(len(word)):\n",
        "            if reverse(word)[i] in ei:\n",
        "                return 'ei'\n",
        "            elif reverse(word)[i] in üö:\n",
        "                return 'üö'\n",
        "            elif reverse(word)[i] in aı:\n",
        "                return 'aı'\n",
        "            elif reverse(word)[i] in uo:\n",
        "                return 'uo'\n",
        "            else:\n",
        "                continue\n",
        "    \n",
        "def StartCheck(word):\n",
        "        ei=['e','i']\n",
        "        aı=['a','ı']\n",
        "        üö=['ü','ö']\n",
        "        uo=['u','o']\n",
        "        for i in range(len(word)):\n",
        "            if (word)[i] in ei:\n",
        "                return 'ei'\n",
        "            elif (word)[i] in üö:\n",
        "                return 'üö'\n",
        "            elif (word)[i] in aı:\n",
        "                return 'aı'\n",
        "            elif (word)[i] in uo:\n",
        "                return 'uo'\n",
        "            else:\n",
        "                continue\n",
        "def replaceall(s, n,a):\n",
        "    occurence = s.count(n)\n",
        "    alt = []\n",
        "    temp = s\n",
        "    for i in range(occurence):\n",
        "        temp2 = temp\n",
        "        for j in range(i,occurence):\n",
        "            temp2 = temp2.replace(n,a,1)\n",
        "            alt.append(temp2)\n",
        "        temp = temp.replace(n,\"!\",1)\n",
        "    for i in range(len(alt)):\n",
        "        alt[i] = alt[i].replace(\"!\",n)\n",
        "\n",
        "    return alt\n",
        "\n",
        "def P(word, N=sum(WORDS.values())):\n",
        "    \"Probability of `word`.\"\n",
        "    if  word in WORDS.keys():\n",
        "        number = WORDS[word]\n",
        "    else:\n",
        "        number = 1\n",
        "    if number == 0:\n",
        "        number = 1\n",
        "    return number / N\n",
        "\n",
        "def correction(word):\n",
        "      return max(candi(word), key=P)\n",
        "\n",
        "def candi(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcçdefgğhıijklmnoöprsştuüvyzw'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        " \n",
        "    sp     = replaceall(word,'ı','i')\n",
        "    sp2     = replaceall(word,'u','ü')\n",
        "    sp3    = replaceall(word,'o','ö')\n",
        "    sp4     = replaceall(word,'g','ğ')\n",
        "    sp5     = replaceall(word,'c','ç')\n",
        "    sp6     = replaceall(word,'s','ş')\n",
        "    sp7     = replaceall(word,'i','ı')\n",
        "    sp8     = replaceall(word,'ö','o')\n",
        "    sp9     = replaceall(word,'ş','s')\n",
        "    sp10     = replaceall(word,'ğ','g')\n",
        "    sp11     = replaceall(word,'ç','c')\n",
        "    sp12     = replaceall(word,'ü','u')\n",
        "    specials=[]\n",
        "    specials.extend(sp)\n",
        "    specials.extend(sp2)\n",
        "    specials.extend(sp3)\n",
        "    specials.extend(sp4)\n",
        "    specials.extend(sp5)\n",
        "    specials.extend(sp6)\n",
        "    specials.extend(sp7)\n",
        "    specials.extend(sp8)\n",
        "    specials.extend(sp9)\n",
        "    specials.extend(sp10)\n",
        "    specials.extend(sp11)\n",
        "    specials.extend(sp12)\n",
        "    return set(deletes+transposes+replaces+inserts+specials)\n",
        "\n",
        "def edits2(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "def print_diff(word, s):\n",
        "    if not word == s:\n",
        "        print(word + \" --> \" + s)\n",
        "counter = 0\n",
        "\n",
        "\n",
        "def lemmatizer(word,texts):\n",
        "        wordList=[]\n",
        "        wordList = re.sub(\"[^\\w]\", \" \",  texts).split()\n",
        "       \n",
        "        pos=wordList.index(word)\n",
        "     \n",
        "        sakin=''\n",
        "        word=correction(word)\n",
        "        \n",
        "        if len(wordList)-pos>3 and pos>2:\n",
        "            for i, kelime in enumerate(wordList[pos-3:pos+4]):\n",
        "                sakin=sakin+correction(kelime)+' '\n",
        "        elif pos<=2 and len(wordList)-pos>5:\n",
        "            for i, kelime in enumerate(wordList[pos:pos+5]):\n",
        "                sakin=sakin+correction(kelime)+' '\n",
        "        elif pos<=2 and len(wordList)-pos<=5:\n",
        "            for i, kelime in enumerate(wordList[pos:len(wordList)]):\n",
        "                sakin=sakin+correction(kelime)+' '\n",
        "        elif len(wordList)-pos<1 and pos>3:\n",
        "            for i, kelime in enumerate(wordList[pos-3:len(wordList)]):\n",
        "                sakin=sakin+correction(kelime)+' '\n",
        "        elif len(wordList)<3:\n",
        "            for i, kelime in enumerate(wordList):\n",
        "                sakin=sakin+correction(kelime)+' '\n",
        "        else:\n",
        "             for i, kelime in enumerate(wordList):\n",
        "                sakin=sakin+correction(kelime)+' '\n",
        "        results = morphology.analyze(word)\n",
        "        lemma=[]\n",
        "        form=[]\n",
        "        l=[]\n",
        "        m=[]\n",
        "        for i, result in enumerate(results):\n",
        "            form.append(str(result.formatLong()))\n",
        "            lemma.append(result.getLemmas()[0])\n",
        "        if len(lemma)>1:\n",
        "                analysis = morphology.analyzeSentence(sakin)\n",
        "                results = morphology.disambiguate(sakin, analysis).bestAnalysis()\n",
        "                for i, result in enumerate(results):\n",
        "                        l.append(result.getLemmas()[0])\n",
        "                        m.append(result.formatLong())\n",
        "                for i in range(len(m)):\n",
        "                    for j in range(len(form)):\n",
        "                        if m[i]==form[j]:\n",
        "                            lema=lemma[j]\n",
        "                            if lema=='değil':\n",
        "                                return 'değil'\n",
        "                            if 'Neg' in form[j] or 'WithoutHavingDoneSo' in form[j] or 'Unable' in form[j]:\n",
        "                                if checkOpennes(word):\n",
        "                                    return lema+'me'\n",
        "                                else:\n",
        "                                    return lema+'ma'\n",
        "                            if 'Without' in form[j]:\n",
        "                                if PresentCheck(word)=='ei':\n",
        "                                    return lema+'siz'\n",
        "                                elif PresentCheck(word)=='aı':\n",
        "                                    return lema+'sız'\n",
        "                                elif PresentCheck(word)=='uo':\n",
        "                                    return lema+'suz'\n",
        "                                else:\n",
        "                                    return lema+'süz'\n",
        "                            if 'With' in form[j]:\n",
        "                                if PresentCheck(word)=='ei':\n",
        "                                    return lema+'li'\n",
        "                                elif PresentCheck(word)=='aı':\n",
        "                                    return lema+'lı'\n",
        "                                elif PresentCheck(word)=='uo':\n",
        "                                    return lema+'lu'\n",
        "                                else:\n",
        "                                    return lema+'lü'\n",
        "                            else:\n",
        "                                return lema\n",
        "                    else:\n",
        "                        continue\n",
        "        elif len(lemma)==1:\n",
        "            if lemma[0]=='değil':\n",
        "                return lemma[0]\n",
        "            if 'Neg' in form[0] or 'WithoutHavingDoneSo' in form[0] or 'Unable' in form[0]:\n",
        "                 if checkOpennes(word):\n",
        "                    return lemma[0]+'me'\n",
        "                 else:\n",
        "                    return lemma[0]+'ma'\n",
        "            elif 'Without' in form[0]:\n",
        "                if PresentCheck(word)=='ei':\n",
        "                    return lemma[0]+'siz'\n",
        "                elif PresentCheck(word)=='aı':\n",
        "                    return lemma[0]+'sız'\n",
        "                elif PresentCheck(word)=='uo':\n",
        "                    return lemma[0]+'suz'\n",
        "                else:\n",
        "                    return lemma[0]+'süz'\n",
        "            elif 'With' in form[0]:\n",
        "                if PresentCheck(word)=='ei':\n",
        "                    return lemma[0]+'li'\n",
        "                elif PresentCheck(word)=='aı':\n",
        "                    return lemma[0]+'lı'\n",
        "                elif PresentCheck(word)=='uo':\n",
        "                    return lemma[0]+'lu'\n",
        "                else:\n",
        "                    return lemma[0]+'lü'\n",
        "            else:\n",
        "                return lemma[0]\n",
        "        else:\n",
        "            return word\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMhMYQ4P6i5Y"
      },
      "source": [
        "def clean(text):\n",
        "    text_list=text.split()\n",
        "    for i, word in enumerate(text_list):\n",
        "        text_list[i] = lemmatizer(word,text)\n",
        "    return ' '.join(''.join(elems) for elems in text_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXd8IqCAovfr"
      },
      "source": [
        "comb[\"clean_new\"]=comb[\"preProcessing\"].map(clean) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeF-Wccua4LV"
      },
      "source": [
        "# 2nd round stopwords ve sil_1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8D1GmL8SurR"
      },
      "source": [
        "comb[\"clean_new\"]=comb[\"clean_new\"].map(sil_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILk4myvGbE2U"
      },
      "source": [
        "comb[\"clean_new\"]=comb[\"clean_new\"].map(stopwords_sil)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxs1SO95cb_u"
      },
      "source": [
        "# Save to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzKLNPOPX4fs"
      },
      "source": [
        "comb.to_csv(r'drive/My Drive/(folder name)/(document name).csv', index=False) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}